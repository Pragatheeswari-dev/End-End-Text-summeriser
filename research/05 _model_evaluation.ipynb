{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/End-End-Text-summeriser'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the config/config.yaml file for model evaluation\n",
    "# evaluate the model and save the metrics in the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. define the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path # root directory of the project\n",
    "    data_path: Path # path to the data\n",
    "    model_path: Path # path to the model\n",
    "    tokenizer_path: Path # path to the tokenizer\n",
    "    metric_file_name: Path # path to the metric file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. configuration manager in src config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummerizer.constants import *\n",
    "from textSummerizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation # get the model evaluation config from the config.yaml file\n",
    "\n",
    "        create_directories([config.root_dir]) # create the directories\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir, # set the root directory\n",
    "            data_path=config.data_path, # set the data path\n",
    "            model_path = config.model_path, # set the model path\n",
    "            tokenizer_path = config.tokenizer_path, # set the tokenizer path\n",
    "            metric_file_name = config.metric_file_name # set the metric file name\n",
    "           \n",
    "        )\n",
    "\n",
    "        return model_evaluation_config # return the model evaluation config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/newtexts/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-19 23:18:22,749: INFO: config: PyTorch version 2.2.1 available.]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model evaluation class\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig): # initialize the model evaluation class\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    # define the method to generate batch sized chunks\n",
    "    def generate_batch_sized_chunks(self,list_of_elements, batch_size):\n",
    "        \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "        Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "        for i in range(0, len(list_of_elements), batch_size):\n",
    "            yield list_of_elements[i : i + batch_size] # yield the batch sized chunks\n",
    "        print(\"Batch sized chunks generated\")\n",
    "\n",
    "    # define the method to calculate the metric on the test dataset\n",
    "    def calculate_metric_on_test_ds(self,dataset, metric, model, tokenizer, \n",
    "                               batch_size=16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "                               column_text=\"article\", \n",
    "                               column_summary=\"highlights\"):\n",
    "        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size)) # generate the article batches \n",
    "        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size)) # generate the target batches\n",
    "        \n",
    "        # iterate through the article and target batches\n",
    "        for article_batch, target_batch in tqdm(\n",
    "            zip(article_batches, target_batches), total=len(article_batches)):\n",
    "            # tokenize the articles\n",
    "            inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n",
    "                            padding=\"max_length\", return_tensors=\"pt\")\n",
    "            # generate the summaries\n",
    "            \n",
    "            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                            attention_mask=inputs[\"attention_mask\"].to(device), \n",
    "                            length_penalty=0.8, num_beams=8, max_length=128)\n",
    "            ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "            \n",
    "            # Finally, we decode the generated texts, \n",
    "            # replace the  token, and add the decoded texts with the references to the metric.\n",
    "            # decode the summaries\n",
    "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n",
    "                                    clean_up_tokenization_spaces=True) \n",
    "                for s in summaries]      \n",
    "            # replace the empty strings with space\n",
    "            decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "            \n",
    "            # add the decoded summaries to the metric\n",
    "            metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "            \n",
    "        #  Finally compute and return the ROUGE scores.\n",
    "        score = metric.compute()\n",
    "        print(\"calculated score:\",score)\n",
    "        return score\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using {device}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n",
    "        print(f\"Tokenizer loaded from {self.config.tokenizer_path}\")\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n",
    "        print(f\"Model loaded from {self.config.model_path}\")\n",
    "        #loading data \n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "        print(f\"Dataset loaded from {self.config.data_path}\")\n",
    "\n",
    "        # define the rouge names\n",
    "        rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "        # load the rouge metric\n",
    "        rouge_metric = load_metric('rouge')\n",
    "\n",
    "        print(\"Calculating ROUGE on test dataset\")\n",
    "\n",
    "        score = self.calculate_metric_on_test_ds(\n",
    "        dataset_samsum_pt['test'][0:10], rouge_metric, model_pegasus, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
    "            )\n",
    "        # create the dataframe\n",
    "        rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names ) # create the rouge dictionary\n",
    "\n",
    "        df = pd.DataFrame(rouge_dict, index = ['pegasus'] )\n",
    "        print(df)\n",
    "        df.to_csv(self.config.metric_file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-19 23:29:33,777: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-19 23:29:33,779: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-19 23:29:33,780: INFO: common: created directory at: artifacts]\n",
      "[2024-03-19 23:29:33,780: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "Using cpu\n",
      "Tokenizer loaded from artifacts/model_trainer/tokenizer\n",
      "Model loaded from artifacts/model_trainer/pegasus-samsum-model\n",
      "Dataset loaded from artifacts/data_transformation/samsum_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19889/2370498068.py:68: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric('rouge')\n",
      "/opt/conda/envs/newtexts/lib/python3.8/site-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 5.65kB [00:00, 12.0MB/s]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE on test dataset\n",
      "Batch sized chunks generated\n",
      "Batch sized chunks generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:28<00:00, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-19 23:31:09,036: INFO: rouge_scorer: Using default tokenizer.]\n",
      "calculated score: {'rouge1': AggregateScore(low=Score(precision=0.0057779353088529325, recall=0.02823763624176721, fmeasure=0.009444431655557705), mid=Score(precision=0.011846186367888152, recall=0.060890209183745256, fmeasure=0.019480017419446032), high=Score(precision=0.017861862880648356, recall=0.0931487725930705, fmeasure=0.02963527480517014)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.0060564010409401604, recall=0.03151548679236182, fmeasure=0.010105231135216617), mid=Score(precision=0.012026005873365655, recall=0.06190476190476191, fmeasure=0.01977648805331053), high=Score(precision=0.018273476150688042, recall=0.09338969918782522, fmeasure=0.03009106532586721)), 'rougeLsum': AggregateScore(low=Score(precision=0.006333662009193292, recall=0.031895768516117876, fmeasure=0.010347295616242942), mid=Score(precision=0.01225324715042407, recall=0.06154120767033864, fmeasure=0.020144626984763984), high=Score(precision=0.018290512626106325, recall=0.09339048239584527, fmeasure=0.030226075934482247))}\n",
      "          rouge1  rouge2    rougeL  rougeLsum\n",
      "pegasus  0.01948     0.0  0.019776   0.020145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager() # create the configuration manager\n",
    "    model_evaluation_config = config.get_model_evaluation_config() # get the model evaluation config\n",
    "    model_evaluation_config = ModelEvaluation(config=model_evaluation_config) # create the model evaluation class\n",
    "    model_evaluation_config.evaluate() # evaluate the model\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newtexts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
