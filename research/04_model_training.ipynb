{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/End-End-Text-summeriser/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-cnn_dailymail\")\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-cnn_dailymail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory to root directory\n",
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/End-End-Text-summeriser'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the config/config.yaml file for model training\n",
    "# update the params.yaml file for model training. \n",
    "# Training parameters for the model. Preparing the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. define the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path # default path to the root directory\n",
    "    data_path: Path # default path to the data directory\n",
    "    model_ckpt: Path # default path to the model checkpoint directory\n",
    "    num_train_epochs: int # default number of training epochs\n",
    "    warmup_steps: int # default number of warmup steps\n",
    "    per_device_train_batch_size: int # default batch size per device during training\n",
    "    weight_decay: float # default weight decay coefficient\n",
    "    logging_steps: int # default number of steps between logging training status\n",
    "    evaluation_strategy: str # default evaluation strategy for evaluation\n",
    "    eval_steps: int # default number of steps between evaluation\n",
    "    save_steps: float # default number of steps between saving checkpoints\n",
    "    gradient_accumulation_steps: int # default number of steps between gradient accumulation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. configuration manager in src config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummerizer.constants import *\n",
    "from textSummerizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "\n",
    "    #constructor assigning values from constant file to the class variables\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        # reading yaml config file and params file and assigning them to the class variables\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        # create directories/file for artifacts\n",
    "        # note that we are using . to access, as we have defined using configbox\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    #  get the model config \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer # get the model config from the config file\n",
    "        params = self.params.TrainingArguments # get the params from the params file\n",
    "\n",
    "        create_directories([config.root_dir]) # create the root directory\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "            num_train_epochs = params.num_train_epochs,\n",
    "            warmup_steps = params.warmup_steps,\n",
    "            per_device_train_batch_size = params.per_device_train_batch_size,\n",
    "            weight_decay = params.weight_decay,\n",
    "            logging_steps = params.logging_steps,\n",
    "            evaluation_strategy = params.evaluation_strategy,\n",
    "            eval_steps = params.eval_steps,\n",
    "            save_steps = params.save_steps,\n",
    "            gradient_accumulation_steps = params.gradient_accumulation_steps\n",
    "        )\n",
    "\n",
    "        return model_trainer_config # return the model config set in the config file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#5. components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/newtexts/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-19 21:01:45,572: INFO: config: PyTorch version 2.2.1 available.]\n"
     ]
    }
   ],
   "source": [
    "# import library for model training\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig): # initialize the model with the given config\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    # train the model\n",
    "    def train(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # select the device to train on gpu or cpu\n",
    "        print(\"device: \", device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt) # initialize the tokenizer with the model checkpoint\n",
    "        print(\"tokenizer: completed\")\n",
    "\n",
    "\n",
    "        print(\"self.config.model_ckpt\",self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device) # initialize the model with the model checkpoint\n",
    "        print(\"model_pegasus loaded\")\n",
    "\n",
    "        \n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus) # initialize the data collator with the tokenizer and model checkpoint for the batches\n",
    "        print(\"seq2seq_data_collator completed \")\n",
    "        #loading data \n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "        print(\"dataset_samsum_pt completed \")\n",
    "\n",
    "        print(\"output_dir=\", self.config.root_dir,\n",
    "            \"num_train_epochs=\", type(self.config.num_train_epochs), \n",
    "            \"warmup_steps=\", type(self.config.warmup_steps),\n",
    "            \"per_device_train_batch_size=\",  type(self.config.per_device_train_batch_size), \n",
    "            \"per_device_eval_batch_size=\", type(self.config.per_device_train_batch_size),\n",
    "            \"weight_decay=\",type(self.config.weight_decay), \n",
    "            \"logging_steps=\",type(self.config.logging_steps),\n",
    "            \"evaluation_strategy=\",type(self.config.evaluation_strategy), \n",
    "            \"eval_steps=\",type(self.config.eval_steps), \n",
    "            \"save_steps=\",type(self.config.save_steps), \n",
    "            \"gradient_accumulation_steps=\", type(self.config.gradient_accumulation_steps))\n",
    "\n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs, \n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size, \n",
    "            per_device_eval_batch_size=self.config.per_device_train_batch_size, \n",
    "            weight_decay=self.config.weight_decay, \n",
    "            logging_steps=self.config.logging_steps,\n",
    "            evaluation_strategy=self.config.evaluation_strategy, \n",
    "            eval_steps=self.config.eval_steps, \n",
    "            save_steps=1e6,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps\n",
    "        ) \n",
    "\n",
    "     \n",
    "        \n",
    "        trainer = Trainer(model=model_pegasus, args=trainer_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"test\"], \n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])\n",
    "\n",
    "          \n",
    "        print(\"training started: \")\n",
    "        trainer.train()\n",
    "        print(\"training completed: \")\n",
    "\n",
    "        ## Save model\n",
    "        model_pegasus.save_pretrained(os.path.join(self.config.root_dir,\"pegasus-samsum-model\"))\n",
    "        print(\"model saved\")\n",
    "        ## Save tokenizer\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))\n",
    "        print(\"tokenizer saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade accelerate 28 torch 2.2.1\n",
    "# pip uninstall -y transformers accelerate\n",
    "# pip install transformers accelerate\n",
    "# pip install transformers[torch] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-19 21:01:45,865: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-19 21:01:45,868: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-19 21:01:45,869: INFO: common: created directory at: artifacts]\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-19 21:01:45,911: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    }
   ],
   "source": [
    "model_trainer_config = config.get_model_trainer_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer_config = ModelTrainer(config=model_trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "tokenizer: completed\n",
      "self.config.model_ckpt google/pegasus-cnn_dailymail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_pegasus loaded\n",
      "seq2seq_data_collator completed \n",
      "dataset_samsum_pt completed \n",
      "output_dir= artifacts/model_trainer num_train_epochs= <class 'int'> warmup_steps= <class 'int'> per_device_train_batch_size= <class 'int'> per_device_eval_batch_size= <class 'int'> weight_decay= <class 'float'> logging_steps= <class 'int'> evaluation_strategy= <class 'str'> eval_steps= <class 'int'> save_steps= <class 'str'> gradient_accumulation_steps= <class 'int'>\n",
      "training started: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/newtexts/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 13:37, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training completed: \n",
      "model saved\n",
      "tokenizer saved\n"
     ]
    }
   ],
   "source": [
    "model_trainer_config.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-19 21:15:52,227: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-19 21:15:52,229: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-19 21:15:52,230: INFO: common: created directory at: artifacts]\n",
      "[2024-03-19 21:15:52,230: INFO: common: created directory at: artifacts/model_trainer]\n",
      "device:  cpu\n",
      "tokenizer: completed\n",
      "self.config.model_ckpt google/pegasus-cnn_dailymail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_pegasus loaded\n",
      "seq2seq_data_collator completed \n",
      "dataset_samsum_pt completed \n",
      "output_dir= artifacts/model_trainer num_train_epochs= <class 'int'> warmup_steps= <class 'int'> per_device_train_batch_size= <class 'int'> per_device_eval_batch_size= <class 'int'> weight_decay= <class 'float'> logging_steps= <class 'int'> evaluation_strategy= <class 'str'> eval_steps= <class 'int'> save_steps= <class 'str'> gradient_accumulation_steps= <class 'int'>\n",
      "training started: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/newtexts/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 13:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training completed: \n",
      "model saved\n",
      "tokenizer saved\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager() # initialize the configuration manager\n",
    "    model_trainer_config = config.get_model_trainer_config() # get the model trainer config from the config file\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config) # initialize the model trainer with the model trainer config\n",
    "    model_trainer_config.train() # train the model\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/transformer from hugging face\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "# create tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "print(\"model_ckpt:\",model_ckpt)\n",
    "print(\"tokenizer:\",tokenizer)\n",
    "\n",
    "print(\"model loading\")\n",
    "\n",
    "# model loading into device\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir, num_train_epochs=self.config.num_train_epochs, warmup_steps=self.config.warmup_steps,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size, per_device_eval_batch_size=self.config.per_device_train_batch_size,\n",
    "            weight_decay=self.config.weight_decay, logging_steps=self.config.logging_steps,\n",
    "            evaluation_strategy=self.config.evaluation_strategy, eval_steps=self.config.eval_steps, save_steps=1e6,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps\n",
    "        ) \n",
    "\n",
    "    trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs, \n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size, \n",
    "            per_device_eval_batch_size=self.config.per_device_train_batch_size,\n",
    "            weight_decay=self.config.weight_decay, \n",
    "            logging_steps=self.config.logging_steps,\n",
    "            evaluation_strategy=self.config.evaluation_strategy, \n",
    "            eval_steps=self.config.eval_steps, \n",
    "            save_steps=1e6,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps\n",
    "        ) \n",
    "           trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir, num_train_epochs=1, warmup_steps=500,\n",
    "            per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "            weight_decay=0.01, logging_steps=10,\n",
    "            evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "            gradient_accumulation_steps=16\n",
    "        ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
